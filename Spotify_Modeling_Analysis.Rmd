---
title: "Spotify Data Modeling"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(caret)
library(dplyr)
library(lmtest)
library(randomForest)
library(gbm)
library(glmnet)
library(e1071)
```

## Initialize Data

```{r}
#Loda Data
data <- readRDS("C:/Users/steph/OneDrive/Documents/Stat447/Final_Project/Spotify_Practice_Data.RDS")
head(data)
summary(data)

#Temp to remove (fixed Popularity_quantized variable - was not including 0 popularity)
data <- data %>% 
  mutate(Popularity_Quantized = cut(Popularity, breaks=c(0, 25, 50, 75, 100), include.lowest = TRUE)) %>%
  select(-Popularity_quantized)

data$Key <- as.factor(data$Key)
data$Mode <- as.factor(data$Mode)

summary(data)

#Split data into training and test sets (80/20 split)
set.seed(447)
samp_size <- floor(0.8 * nrow(data))
idx <- sample(seq_len(nrow(data)), size = samp_size)

train <- data[idx,]
test <- data[-idx,]

#Check final dimensions for training and testing set
dim(train)
dim(test)
```

## Regression Models

```{r}
reg_train <- train[,4:16]
reg_test <- test[,4:16]

reg_test_x <- reg_test[, -1] 
reg_test_y <- as.matrix(reg_test[, 1], nrow = nrow(reg_test[, 1]), ncol = 1)
```

### 1) Random Forest

```{r}
#Fit random forest regression model (update ntree = 800-1000 on final dataset)
rf_reg_fit <- randomForest(Popularity ~ ., data = reg_train, ntree = 10,
                           importance = TRUE)
rf_reg_fit

#Tune model (later)

#Function that computes RMSE and R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE <- sqrt(SSE/nrow(df))

  #Summarize model performance metrics
  data.frame(
    RMSE <- RMSE,
    Rsquare <- R_square
  )
}

#Prediction and evaluation on test data
rf_reg_pred_y <- predict(rf_reg_fit, reg_test_x)
rf_reg_res <- eval_results(as.matrix(reg_test[, 1]), rf_reg_pred_y, as.matrix(reg_test))

#Get variable importance from the model fit
ImpData <- as.data.frame(importance(rf_reg_fit))
ImpData$Var.Names <- row.names(ImpData)

ggplot(ImpData, aes(x = Var.Names, y = `%IncMSE`)) +
  geom_segment(aes(x = Var.Names, xend = Var.Names, y = 0, yend = `%IncMSE`), color = "black") +
  geom_point(aes(size = IncNodePurity), color = "blue", alpha = 0.6) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position = "bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )
```

### 2) Gradient Boosting Machine

```{r}
#Fit GBM
tc <- trainControl(method = "cv", number = 10)
gbm_reg_fit <- train(Popularity ~., data = reg_train, method = "gbm", 
                     trControl = tc, verbose = FALSE)
gbm_reg_fit

#Tune model (later)

#Prediction and evaluation on test data
gbm_reg_pred_y <- predict(gbm_reg_fit, reg_test_x)
gbm_reg_res <- eval_results(as.matrix(reg_test[, 1]), gbm_reg_pred_y, as.matrix(reg_test))
```

### 3) Ridge Regression

```{r}
#Scale numeric predictors
reg_train_sc <- reg_train
reg_test_sc <- reg_test

#Convert factors to dummy vars
reg_train_sc_dummy <- data.frame(reg_train_sc[ ,!colnames(reg_train_sc) %in% "Key"],
                         model.matrix( ~ Key - 1, reg_train_sc))
reg_train_sc_dummy <- data.frame(reg_train_sc_dummy[ ,!colnames(reg_train_sc_dummy) %in% "Mode"],
                         model.matrix( ~ Mode - 1, reg_train_sc_dummy))
reg_test_sc_dummy <- data.frame(reg_test_sc[ ,!colnames(reg_test_sc) %in% "Key"],
                         model.matrix( ~ Key - 1, reg_test_sc))
reg_test_sc_dummy <- data.frame(reg_test_sc_dummy[ ,!colnames(reg_test_sc_dummy) %in% "Mode"],
                         model.matrix( ~ Mode - 1, reg_test_sc_dummy))

pre_proc_val <- preProcess(reg_train_sc_dummy[, 2:13], method = c("center", "scale"))

reg_train_sc_dummy[, 2:13] <- predict(pre_proc_val, reg_train_sc_dummy[, 2:13])
reg_test_sc_dummy[, 2:13] <- predict(pre_proc_val, reg_test_sc_dummy[, 2:13])

summary(reg_train_sc_dummy)

#Fit ridge regression

#Find optimal lambda
lambdas <- 10^seq(2, -3, by = -.1)
cv_ridge <- cv.glmnet(x = as.matrix(reg_train_sc_dummy[, 2:13]), 
                                    y = reg_train_sc_dummy[, 1], alpha = 0, 
                                    lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda

# ridge_reg_fit <- glmnet(as.matrix(reg_train_sc_dummy[, 2:13]), 
#                         reg_train_sc_dummy[, 1], nlambda = 25, alpha = 0, 
#                         family = 'gaussian', lambda = lambdas)

# summary(ridge_reg_fit)

#Prediction and evaluation on test data
ridge_pred_y <- predict(cv_ridge, s = optimal_lambda, newx = as.matrix(reg_test_sc_dummy[, 2:13]))
ridge_reg_res <- eval_results(reg_test_sc_dummy[, 1], ridge_pred_y, reg_test_sc_dummy)
```

### 4) Lasso Regression

```{r}
#Fit lasso regression

#Find optimal lambda
lambdas <- 10^seq(2, -3, by = -.1)

cv_lasso <- cv.glmnet(x = as.matrix(reg_train_sc_dummy[, 2:13]), 
                      y = reg_train_sc_dummy[, 1], alpha = 0, 
                      lambda = lambdas, standardize = TRUE, nfolds = 5)
optimal_lambda <- cv_lasso$lambda.min 
optimal_lambda

#Prediction and evaluation on test data
lasso_pred_y <- predict(cv_lasso, s = optimal_lambda, newx = as.matrix(reg_test_sc_dummy[, 2:13]))
lasso_reg_res <- eval_results(reg_test_sc_dummy[, 1], lasso_pred_y, reg_test_sc_dummy)
```

### 5) Support Vector Regression

```{r}
#Fit support vector regression
svm_reg_fit <- svm(Popularity~., data = reg_train)

#Tune model (later)

#Prediction and evaluation on test data
svm_reg_pred_y <- predict(svm_reg_fit, newx = reg_test_x, newdata = reg_test)
svm_reg_res <- eval_results(as.matrix(reg_test[, 1]), svm_reg_pred_y, as.matrix(reg_test))
```

### Summarize Results

```{r}
#Make a table of regression results
tab <- matrix(c(rf_reg_res, gbm_reg_res, ridge_reg_res, lasso_reg_res, 
                svm_reg_res), nrow = 5, ncol = 2, byrow = TRUE)
rownames(tab) <- c("Random Forest", "GBM", "Ridge", "Lasso", "SVM")
colnames(tab) <- c("RMSE", "R^2")
tab
```

## Classification Models

```{r}
cl_train <- train[,5:17]
cl_test <- test[,5:17]

cl_test_x <- cl_test[, -13] 
cl_test_y <- as.matrix(cl_test[, 13], nrow = nrow(cl_test[, 13]), ncol = 1)
```

### 1) Random Forest

```{r}
#Fit random forest regression model (update ntree = 800-1000 on final dataset)
rf_cl_fit <- randomForest(Popularity_Quantized ~ ., data = cl_train, ntree = 10,
                           importance = TRUE)
rf_cl_fit

#Tune model (later)

#Prediction and evaluation on test data
rf_cl_pred_y <- predict(rf_cl_fit, cl_test_x)
table <- table(rf_cl_pred_y, cl_test_y)[1:4, c(4,1:3)]
table
rf_table <- confusionMatrix(table)

#Get variable importance from the model fit
ImpData <- as.data.frame(importance(rf_cl_fit))
ImpData$Var.Names <- row.names(ImpData)

ggplot(ImpData, aes(x = Var.Names, y = `MeanDecreaseAccuracy`)) +
  geom_segment(aes(x = Var.Names, xend = Var.Names, y = 0, yend = `MeanDecreaseAccuracy`), color = "black") +
  theme_light() +
  coord_flip() +
  theme(
    legend.position = "bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )
```

### 2) Gradient Boosting Machine

```{r}
#Fit GBM
tc <- trainControl(method = "cv", number = 10)
gbm_cl_fit <- train(Popularity_Quantized ~., data = cl_train, method = "gbm", 
                     trControl = tc, verbose = FALSE)
gbm_cl_fit

#Tune model (later)

#Prediction and evaluation on test data
gbm_cl_pred_y <- predict(gbm_cl_fit, cl_test_x)
table <- table(gbm_cl_pred_y, cl_test_y)[1:4, c(4,1:3)]
table
gbm_table <- confusionMatrix(table)
```

### 3) K-Nearest Neighbors

```{r}
#Scale numeric predictors
cl_train_sc <- cl_train
cl_test_sc <- cl_test

#Convert factors to dummy vars
cl_train_sc_dummy <- data.frame(cl_train_sc[ ,!colnames(cl_train_sc) %in% "Key"],
                         model.matrix( ~ Key - 1, cl_train_sc))
cl_train_sc_dummy <- data.frame(cl_train_sc_dummy[ ,!colnames(cl_train_sc_dummy) %in% "Mode"],
                         model.matrix( ~ Mode - 1, cl_train_sc_dummy))
cl_test_sc_dummy <- data.frame(cl_test_sc[ ,!colnames(cl_test_sc) %in% "Key"],
                         model.matrix( ~ Key - 1, cl_test_sc))
cl_test_sc_dummy <- data.frame(cl_test_sc_dummy[ ,!colnames(cl_test_sc_dummy) %in% "Mode"],
                         model.matrix( ~ Mode - 1, cl_test_sc_dummy))

pre_proc_val <- preProcess(cl_train_sc_dummy[, c(1:10, 12:25)], method = c("center", "scale"))

cl_train_sc_dummy[, c(1:10, 12:25)] <- predict(pre_proc_val, cl_train_sc_dummy[, c(1:10, 12:25)])
cl_test_sc_dummy[, c(1:10, 12:25)] <- predict(pre_proc_val, cl_test_sc_dummy[, c(1:10, 12:25)])

summary(cl_train_sc_dummy)

#Fit KNN
knn_cl_fit <- train(cl_train_sc_dummy[, c(1:10, 12:25)], cl_train_sc_dummy[, 11], method = "knn", preProcess = c("center","scale"))
knn_cl_fit

#Tune model (later)

#Prediction and evaluation on test data
knn_cl_pred_y <- predict(knn_cl_fit, cl_test_sc_dummy[, c(1:10, 12:25)])
table <- table(knn_cl_pred_y, cl_test_sc_dummy[, 11])
table
knn_table <- confusionMatrix(table)
```

### 4) Linear Discriminant Analysis

```{r}
#Fit LDA
tc <- trainControl(method = "cv", number = 10)
lda_fit <- train(Popularity_Quantized ~., data = cl_train, method = "lda", 
              trControl = tc, metric = "Accuracy")
lda_fit

#Tune model (later)

#Prediction and evaluation on test data
lda_pred_y <- predict(lda_fit, cl_test_x)
table <- table(lda_pred_y, cl_test_y)[1:4, c(4,1:3)]
table
lda_table <- confusionMatrix(table)
```

### 5) Support Vector Classifier

```{r}
#Fit SVM
svm_cl_fit <- svm(Popularity_Quantized ~., data = cl_train, 
          method = "C-classification", kernal = "radial")
svm_cl_fit

#Tune model (later)

#Prediction and evaluation on test data
svm_cl_pred_y <- predict(svm_cl_fit, cl_test_x)
table <- table(svm_cl_pred_y, cl_test_y)[1:4, c(4,1:3)]
table
svm_table <- confusionMatrix(table)
```

### Summarize Results

```{r}
#Make a table of classification results
tab <- matrix(c(rf_table$overall[1], gbm_table$overall[1], knn_table$overall[1],
                lda_table$overall[1], svm_table$overall[1]), 
              nrow = 5, ncol = 1, byrow = TRUE)
rownames(tab) <- c("Random Forest", "GBM", "KNN", "LDA", "SVM")
colnames(tab) <- c("Accuracy")
tab
```
