---
title: "Spotify Data Modeling"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(caret)
library(dplyr)
library(lmtest)
library(randomForest)
library(gbm)
library(glmnet)
library(e1071)
library(ranger)
library(abcrf)
library(kernlab)
library(formattable)
```

## Initialize Data

```{r}
#Initialize Data
data <- readRDS("~/spotify_combined_data.RDS")
summary(data)
colSums(is.na(data))

#Remove NA rows
data <- data[complete.cases(data),]
summary(data)
colSums(is.na(data))

#Check class types of all variables
lapply(data, class)

#Convert character features to factors
data$Key <- as.factor(data$Key)
data$Mode <- as.factor(data$Mode)

summary(data)

#Randomly sample 800,000 observations from full dataset for run time purposes
set.seed(1221)
obs <- sample(1:nrow(data), 80000)
subdata <- data[obs,]

#Split data into training and test sets (80/20 split)
set.seed(447)
samp_size <- floor(0.8 * nrow(subdata))
idx <- sample(seq_len(nrow(subdata)), size = samp_size)

train <- subdata[idx,]
test <- subdata[-idx,]

#Check final dimensions for training and testing set
dim(train)
dim(test)
```

## Regression Models

```{r}
reg_train <- train[,4:16]
reg_test <- test[,4:16]

reg_test_x <- reg_test[, -1] 
reg_test_y <- as.matrix(reg_test[, 1], nrow = nrow(reg_test[, 1]), ncol = 1)
```

### 1) Random Forest

```{r}
#Fit random forest regression model
rf_reg_fit <- ranger(Popularity ~ ., data = reg_train, 
                     importance = 'impurity')
rf_reg_fit

#Function that computes RMSE and R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE <- sqrt(SSE/nrow(df))

  #Summarize model performance metrics
  data.frame(
    RMSE <- RMSE,
    Rsquare <- R_square
  )
}

#Prediction and evaluation on test data
rf_reg_pred_y <- predict(rf_reg_fit, reg_test_x)
rf_reg_res <- eval_results(as.matrix(reg_test[, 1]), rf_reg_pred_y$predictions, as.matrix(reg_test))

#Get variable importance from the model fit
importance <- as.vector(rf_reg_fit$variable.importance)
variable <- as.vector(colnames(reg_train)[2:13])
var_imp <- cbind(variable, importance)
var_imp <- as.data.frame(var_imp)
var_imp$importance <- as.numeric(var_imp$importance)
var_imp

ggplot(var_imp, aes(x=reorder(variable,importance), y=importance, fill=importance))+ 
      geom_bar(stat="identity", position="dodge")+ coord_flip()+
      ylab("Variable Importance")+
      xlab("")+
      ggtitle("Variable Importance Plot")

#Tune mtry parameter (# of vars to randomly sample as candidates at each split)
rf_grid <- expand.grid(mtry = c(2:12), splitrule = "variance", min.node.size = 5)

fitControl <- trainControl(method = "CV",
                           number = 5,
                           verboseIter = TRUE)

rf_reg_tune <- train(x = reg_train[,2:13], y = reg_train[,1],
                        method = 'ranger',
                        tuneGrid = rf_grid,
                        metric = "RMSE",
                        trControl = fitControl)
rf_reg_tune

plot(rf_reg_tune$results[,1], rf_reg_tune$results[,4], xlab = "Mtry", ylab = "RMSE",
     type = "l")

#Optimal model is mtry = 3 which is the random forest regression model that was already run
```

### 2) Gradient Boosting Machine

```{r}
#Fit GBM
gbm_reg_fit <- train(Popularity ~., data = reg_train, method = "gbm",
                     trControl = fitControl, verbose = FALSE)
gbm_reg_fit

#Prediction and evaluation on test data
gbm_reg_pred_y <- predict(gbm_reg_fit, reg_test_x)
gbm_reg_res <- eval_results(as.matrix(reg_test[, 1]), gbm_reg_pred_y, as.matrix(reg_test))

#Tune interaction.depth (tree complexity) and n.trees (# of iterations) parameters
gbm_grid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                        n.trees = (1:20)*50, 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)

gbm_reg_tune <- train(Popularity ~., data = reg_train, method = "gbm",
                     trControl = fitControl, verbose = FALSE, tuneGrid = gbm_grid)
gbm_reg_tune

plot(gbm_reg_tune)

gbm_grid_tune <-  expand.grid(interaction.depth = 9, 
                        n.trees = 400, 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)

gbm_reg_tuned_fit <- train(Popularity ~., data = reg_train, method = "gbm",
                     trControl = fitControl, verbose = FALSE, tuneGrid = gbm_grid_tune)
gbm_reg_tuned_fit

#Tuned prediction and evaluation on test data
gbm_reg_tuned_pred_y <- predict(gbm_reg_tuned_fit, reg_test_x)
gbm_reg_tuned_res <- eval_results(as.matrix(reg_test[, 1]), gbm_reg_tuned_pred_y, as.matrix(reg_test))
```

### 3) Support Vector Regression

```{r}
#Fit support vector regression
svm_reg_fit <- train(Popularity ~., data = reg_train, method = "svmLinear", 
                     trControl = fitControl,  preProcess = c("center","scale"))
svm_reg_fit

#Prediction and evaluation on test data
svm_reg_pred_y <- predict(svm_reg_fit, reg_test_x)
svm_reg_res <- eval_results(as.matrix(reg_test[, 1]), svm_reg_pred_y, as.matrix(reg_test))

#Tune C parameter (cost - penalty to model for making error)
svm_grid <- expand.grid(C = c(0.25, .5, 1))

svm_reg_tune <- train(Popularity ~., data = reg_train, method = "svmLinear", 
                           trControl = fitControl,  preProcess = c("center","scale"),
                           tuneGrid = svm_grid)
svm_reg_tune

plot(svm_reg_tune)

svm_grid_tune <- expand.grid(C = 0.25)

svm_reg_tuned_fit <- train(Popularity ~., data = reg_train, method = "svmLinear", 
                     trControl = fitControl,  preProcess = c("center","scale"),
                     tuneGrid = svm_grid_tune)
svm_reg_tuned_fit

#Tuned prediction and evaluation on test data
svm_reg_tuned_pred_y <- predict(svm_reg_tuned_fit, reg_test_x)
svm_reg_tuned_res <- eval_results(as.matrix(reg_test[, 1]), svm_reg_tuned_pred_y, as.matrix(reg_test))
```

### Summarize Results

```{r}
#Make a table of regression results
tab <- matrix(c(round(rf_reg_res,3), round(rf_reg_res,3), round(gbm_reg_res,3), 
                round(gbm_reg_tuned_res,3), round(svm_reg_res,3), 
                round(svm_reg_tuned_res,3)), nrow = 6, ncol = 2, byrow = TRUE)
rownames(tab) <- c("Random Forest", "Random Forest Tuned", "GBM", "GBM Tuned", "SVM",
                   "SVM Tuned")
colnames(tab) <- c("RMSE", "R^2")
tab
```

## Classification Models

```{r}
cl_train <- train[,5:17]
cl_test <- test[,5:17]

cl_test_x <- cl_test[, -13] 
cl_test_y <- as.matrix(cl_test[, 13], nrow = nrow(cl_test[, 13]), ncol = 1)
```

### 1) Random Forest

```{r}
#Fit random forest regression model
rf_cl_fit <- ranger(Popularity_quantized ~ ., data = cl_train, 
                     importance = 'impurity')
rf_cl_fit

#Prediction and evaluation on test data
rf_cl_pred_y <- predict(rf_cl_fit, cl_test_x)
table <- table(rf_cl_pred_y$predictions, cl_test_y)[1:4, c(4,1:3)]
table
rf_table <- confusionMatrix(table)

#Get variable importance from the model fit
importance <- as.vector(rf_cl_fit$variable.importance)
variable <- as.vector(colnames(cl_train)[1:12])
var_imp <- cbind(variable, importance)
var_imp <- as.data.frame(var_imp)
var_imp$importance <- as.numeric(var_imp$importance)
var_imp

ggplot(var_imp, aes(x=reorder(variable,importance), y=importance, fill=importance))+ 
      geom_bar(stat="identity", position="dodge")+ coord_flip()+
      ylab("Variable Importance")+
      xlab("")+
      ggtitle("Variable Importance Plot")

#Tune mtry parameter (# of vars to randomly sample as candidates at each split)
rf_grid <- expand.grid(mtry = c(2:12), splitrule = "gini", min.node.size = 1)

fitControl <- trainControl(method = "CV",
                           number = 5,
                           verboseIter = TRUE)

rf_cl_tune <- train(x = cl_train[,1:12], y = cl_train[,13],
                        method = 'ranger',
                        tuneGrid = rf_grid,
                        metric = "Accuracy",
                        trControl = fitControl)
rf_cl_tune

plot(rf_cl_tune$results[,1], rf_cl_tune$results[,4], xlab = "Mtry", ylab = "Accuracy",
     type = "l")

#Optimal model is mtry = 3 which is the random forest classification model that was already run
```

### 2) Gradient Boosting Machine

```{r}
#Fit GBM
gbm_cl_fit <- train(Popularity_quantized ~., data = cl_train, method = "gbm",
                     trControl = fitControl, verbose = FALSE)
gbm_cl_fit

#Prediction and evaluation on test data
gbm_cl_pred_y <- predict(gbm_cl_fit, cl_test_x)
table <- table(gbm_cl_pred_y, cl_test_y)[1:4, c(4,1:3)]
table
gbm_table <- confusionMatrix(table)

#Tune interaction.depth (tree complexity) and n.trees (# of iterations) parameters
gbm_grid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                        n.trees = (1:20)*50, 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)

gbm_cl_tune <- train(Popularity_quantized ~., data = cl_train, method = "gbm",
                     trControl = fitControl, verbose = FALSE, tuneGrid = gbm_grid)
gbm_cl_tune

plot(gbm_cl_tune)

gbm_grid_tune <-  expand.grid(interaction.depth = 9, 
                        n.trees = 100, 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)

gbm_cl_tuned_fit <- train(Popularity_quantized ~., data = cl_train, method = "gbm",
                     trControl = fitControl, verbose = FALSE, tuneGrid = gbm_grid_tune)
gbm_cl_tuned_fit

#Tuned prediction and evaluation on test data
gbm_cl_tuned_pred_y <- predict(gbm_cl_tuned_fit, cl_test_x)
table <- table(gbm_cl_tuned_pred_y, cl_test_y)[1:4, c(4,1:3)]
table
gbm_tuned_table <- confusionMatrix(table)
```

### 3) Support Vector Classifier

```{r}
#Fit support vector regression
svm_cl_fit <- train(Popularity_quantized ~., data = cl_train, method = "svmLinear", 
                     trControl = fitControl,  preProcess = c("center","scale"))
svm_cl_fit

#Prediction and evaluation on test data
svm_cl_pred_y <- predict(svm_cl_fit, cl_test_x)
table <- table(svm_cl_pred_y, cl_test_y)[1:4, c(4,1:3)]
table
svm_table <- confusionMatrix(table)

#Tune C parameter (cost - penalty to model for making error)
svm_grid <- expand.grid(C = c(0.25, .5, 1))

svm_cl_tune <- train(Popularity_quantized ~., data = cl_train, method = "svmLinear", 
                           trControl = fitControl,  preProcess = c("center","scale"),
                           tuneGrid = svm_grid)
svm_cl_tune

plot(svm_cl_tune)

#Optimal model is C = 1 which is the svm classification model that was already run
```

### Summarize Results

```{r}
#Make a table of classification results
tab <- matrix(c(round(rf_table$overall[1],3), round(rf_table$overall[1],3), 
                round(gbm_table$overall[1],3), round(gbm_tuned_table$overall[1],3), 
                round(svm_table$overall[1],3), round(svm_table$overall[1],3)), 
              nrow = 6, ncol = 1, byrow = TRUE)
rownames(tab) <- c("Random Forest", "Random Forest Tuned", "GBM", "GBM Tuned", "SVM",
                   "SVM Tuned")
colnames(tab) <- c("Accuracy")
tab
```
